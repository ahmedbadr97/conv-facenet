{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.efficentnet_train import data_load,models_train,visualization,utils,generate_dataset,evaluation\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from src.efficentfacenet import face_descriptor\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = \"../dataset/preprocessed\"\n",
    "model_weights_path=\"../model_weights/training\"\n",
    "train_data_save_path=\"../training log\"\n",
    "batch_size=8\n",
    "stop_n_layers=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train_history=pd.read_csv(train_data_save_path+\"/train_data.csv\",index_col=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "      Train Loss  no train rows  Test Loss  No test rows  Time taken (M)  \\\n0       0.280173         100000   0.342320         25000            65.2   \n1       0.989897         100000   1.000320         25000            60.1   \n2       1.000000         100000   1.000320         25000            60.2   \n3       0.173325          50000   0.284327         10000            63.0   \n4       0.243919          50000   0.281481         10000            60.0   \n5       0.210516          50000   0.238477         10000            59.9   \n6       0.122000          50000   0.259760         10000            62.9   \n7       0.197626          50000   0.260942         10000            60.1   \n8       0.168668          50000   0.216822         10000            60.2   \n9       0.097149          25000   0.268254          5000            31.3   \n10      0.348466          50000   0.329321         10000            77.7   \n11      0.195241          50000   0.275927         10000            83.4   \n12      0.996439          50000   1.000000         10000            79.3   \n13      1.000000          50000   1.000000         10000            79.3   \n14      1.000000          50000   1.000000         10000            79.4   \n15      1.000000          50000   1.000000         10000            79.6   \n16      0.652350           5000   0.515885         10000            12.7   \n17  31245.067151           5000   1.000000         10000            12.3   \n18      1.000000           5000   1.000000         10000            12.5   \n19      1.000000           5000   1.000000         10000            12.5   \n20      1.000000           5000   1.000000         10000            12.4   \n21      0.688501           5000   0.508890         10000            11.9   \n22    557.757865           5000   1.000000         10000            11.3   \n23      1.000000           5000   1.000000         10000            11.0   \n24      1.000000           5000   1.000000         10000            10.9   \n25      1.000000           5000   1.000000         10000            10.9   \n26      1.000000           5000   1.000000         10000            11.0   \n27      1.000000           5000   1.000000         10000            11.1   \n28      1.000000           5000   1.000000         10000            11.0   \n29      1.000000           5000   1.000000         10000            11.0   \n30      1.000000           5000   1.000000         10000            11.0   \n31      0.468498          25000   0.351068          5000            36.2   \n32      1.876949          25000   1.000000          5000            33.3   \n33      0.797767           5000   0.654682          5000             9.4   \n34      0.990279           5000   1.000000          5000             8.5   \n35      1.000000           5000   1.000000          5000             8.5   \n36      1.000000           5000   1.000000          5000             8.5   \n37      1.000000           5000   1.000000          5000             8.5   \n38      1.000000           5000   1.000000          5000             8.5   \n39      1.000000           5000   1.000000          5000             8.5   \n40      1.000000           5000   1.000000          5000             8.5   \n41      1.000000           5000   1.000000          5000             8.5   \n42      1.000000           5000   1.000000          5000             8.5   \n43      0.584966          25000   0.429947          5000            67.8   \n\n                                                Notes        Date      Time  \n0   remove sigmoid from classifier + input normali...  15/05/2022  23:26:00  \n1   remove sigmoid from classifier + input normali...  16/05/2022  00:27:00  \n2   remove sigmoid from classifier + input normali...  16/05/2022  01:27:00  \n3   remove sigmoid from classifier + input normali...  16/05/2022  19:28:00  \n4   remove sigmoid from classifier + input normali...  16/05/2022  20:28:00  \n5   remove sigmoid from classifier + input normali...  16/05/2022  21:28:00  \n6   remove sigmoid from classifier + input normali...  16/05/2022  22:38:00  \n7   remove sigmoid from classifier + input normali...  16/05/2022  23:38:00  \n8   remove sigmoid from classifier + input normali...  17/05/2022  00:38:00  \n9     Add sigmoid to classifier + input normalization  17/05/2022  14:29:00  \n10  Classifier -->(1280,128,sigmoid) + input stand...  17/05/2022  18:32:00  \n11  Classifier -->(1280,128,sigmoid) + input stand...  17/05/2022  20:02:00  \n12  Classifier -->(1280,128,sigmoid) + input stand...  17/05/2022  21:22:00  \n13  Classifier -->(1280,128,sigmoid) + input stand...  17/05/2022  22:41:00  \n14  Classifier -->(1280,128,sigmoid) + input stand...  18/05/2022  00:00:00  \n15  Classifier -->(1280,128,sigmoid) + input stand...  18/05/2022  01:20:00  \n16  Classifier -->(1280,128) + input standardizati...  18/05/2022  02:33:00  \n17  Classifier -->(1280,128) + input standardizati...  18/05/2022  02:45:00  \n18  Classifier -->(1280,128) + input standardizati...  18/05/2022  02:58:00  \n19  Classifier -->(1280,128) + input standardizati...  18/05/2022  03:10:00  \n20  Classifier -->(1280,128) + input standardizati...  18/05/2022  03:23:00  \n21  Classifier -->(1280,128) + input standardizati...  18/05/2022  03:39:00  \n22  Classifier -->(1280,128) + input standardizati...  18/05/2022  03:50:00  \n23  Classifier -->(1280,128) + input standardizati...  18/05/2022  04:01:00  \n24  Classifier -->(1280,128) + input standardizati...  18/05/2022  04:12:00  \n25  Classifier -->(1280,128) + input standardizati...  18/05/2022  04:23:00  \n26  Classifier -->(1280,128) + input standardizati...  18/05/2022  04:34:00  \n27  Classifier -->(1280,128) + input standardizati...  18/05/2022  04:45:00  \n28  Classifier -->(1280,128) + input standardizati...  18/05/2022  04:56:00  \n29  Classifier -->(1280,128) + input standardizati...  18/05/2022  05:07:00  \n30  Classifier -->(1280,128) + input standardizati...  18/05/2022  05:18:00  \n31  Classifier -->(1280,128) + input standardizati...  18/05/2022  07:17:00  \n32  Classifier -->(1280,128) + input standardizati...  18/05/2022  07:51:00  \n33  Classifier -->(1280,128) + input standardizati...  18/05/2022  08:17:00  \n34  Classifier -->(1280,128) + input standardizati...  18/05/2022  08:26:00  \n35  Classifier -->(1280,128) + input standardizati...  18/05/2022  08:34:00  \n36  Classifier -->(1280,128) + input standardizati...  18/05/2022  08:43:00  \n37  Classifier -->(1280,128) + input standardizati...  18/05/2022  08:51:00  \n38  Classifier -->(1280,128) + input standardizati...  18/05/2022  09:00:00  \n39  Classifier -->(1280,128) + input standardizati...  18/05/2022  09:08:00  \n40  Classifier -->(1280,128) + input standardizati...  18/05/2022  09:17:00  \n41  Classifier -->(1280,128) + input standardizati...  18/05/2022  09:25:00  \n42  Classifier -->(1280,128) + input standardizati...  18/05/2022  09:34:00  \n43  Classifier -->(1280,512,128)+ sigmoid + input ...  18/05/2022  12:33:00  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Train Loss</th>\n      <th>no train rows</th>\n      <th>Test Loss</th>\n      <th>No test rows</th>\n      <th>Time taken (M)</th>\n      <th>Notes</th>\n      <th>Date</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.280173</td>\n      <td>100000</td>\n      <td>0.342320</td>\n      <td>25000</td>\n      <td>65.2</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>15/05/2022</td>\n      <td>23:26:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.989897</td>\n      <td>100000</td>\n      <td>1.000320</td>\n      <td>25000</td>\n      <td>60.1</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>16/05/2022</td>\n      <td>00:27:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.000000</td>\n      <td>100000</td>\n      <td>1.000320</td>\n      <td>25000</td>\n      <td>60.2</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>16/05/2022</td>\n      <td>01:27:00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.173325</td>\n      <td>50000</td>\n      <td>0.284327</td>\n      <td>10000</td>\n      <td>63.0</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>16/05/2022</td>\n      <td>19:28:00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.243919</td>\n      <td>50000</td>\n      <td>0.281481</td>\n      <td>10000</td>\n      <td>60.0</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>16/05/2022</td>\n      <td>20:28:00</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.210516</td>\n      <td>50000</td>\n      <td>0.238477</td>\n      <td>10000</td>\n      <td>59.9</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>16/05/2022</td>\n      <td>21:28:00</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.122000</td>\n      <td>50000</td>\n      <td>0.259760</td>\n      <td>10000</td>\n      <td>62.9</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>16/05/2022</td>\n      <td>22:38:00</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.197626</td>\n      <td>50000</td>\n      <td>0.260942</td>\n      <td>10000</td>\n      <td>60.1</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>16/05/2022</td>\n      <td>23:38:00</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.168668</td>\n      <td>50000</td>\n      <td>0.216822</td>\n      <td>10000</td>\n      <td>60.2</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>17/05/2022</td>\n      <td>00:38:00</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.097149</td>\n      <td>25000</td>\n      <td>0.268254</td>\n      <td>5000</td>\n      <td>31.3</td>\n      <td>Add sigmoid to classifier + input normalization</td>\n      <td>17/05/2022</td>\n      <td>14:29:00</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.348466</td>\n      <td>50000</td>\n      <td>0.329321</td>\n      <td>10000</td>\n      <td>77.7</td>\n      <td>Classifier --&gt;(1280,128,sigmoid) + input stand...</td>\n      <td>17/05/2022</td>\n      <td>18:32:00</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.195241</td>\n      <td>50000</td>\n      <td>0.275927</td>\n      <td>10000</td>\n      <td>83.4</td>\n      <td>Classifier --&gt;(1280,128,sigmoid) + input stand...</td>\n      <td>17/05/2022</td>\n      <td>20:02:00</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.996439</td>\n      <td>50000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>79.3</td>\n      <td>Classifier --&gt;(1280,128,sigmoid) + input stand...</td>\n      <td>17/05/2022</td>\n      <td>21:22:00</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1.000000</td>\n      <td>50000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>79.3</td>\n      <td>Classifier --&gt;(1280,128,sigmoid) + input stand...</td>\n      <td>17/05/2022</td>\n      <td>22:41:00</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1.000000</td>\n      <td>50000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>79.4</td>\n      <td>Classifier --&gt;(1280,128,sigmoid) + input stand...</td>\n      <td>18/05/2022</td>\n      <td>00:00:00</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1.000000</td>\n      <td>50000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>79.6</td>\n      <td>Classifier --&gt;(1280,128,sigmoid) + input stand...</td>\n      <td>18/05/2022</td>\n      <td>01:20:00</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.652350</td>\n      <td>5000</td>\n      <td>0.515885</td>\n      <td>10000</td>\n      <td>12.7</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>02:33:00</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>31245.067151</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>12.3</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>02:45:00</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>12.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>02:58:00</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>12.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>03:10:00</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>12.4</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>03:23:00</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.688501</td>\n      <td>5000</td>\n      <td>0.508890</td>\n      <td>10000</td>\n      <td>11.9</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>03:39:00</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>557.757865</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>11.3</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>03:50:00</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>11.0</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>04:01:00</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>10.9</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>04:12:00</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>10.9</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>04:23:00</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>11.0</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>04:34:00</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>11.1</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>04:45:00</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>11.0</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>04:56:00</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>11.0</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>05:07:00</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>11.0</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>05:18:00</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.468498</td>\n      <td>25000</td>\n      <td>0.351068</td>\n      <td>5000</td>\n      <td>36.2</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>07:17:00</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>1.876949</td>\n      <td>25000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>33.3</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>07:51:00</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.797767</td>\n      <td>5000</td>\n      <td>0.654682</td>\n      <td>5000</td>\n      <td>9.4</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>08:17:00</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.990279</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>08:26:00</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>08:34:00</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>08:43:00</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>08:51:00</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>09:00:00</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>09:08:00</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>09:17:00</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>09:25:00</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>09:34:00</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>0.584966</td>\n      <td>25000</td>\n      <td>0.429947</td>\n      <td>5000</td>\n      <td>67.8</td>\n      <td>Classifier --&gt;(1280,512,128)+ sigmoid + input ...</td>\n      <td>18/05/2022</td>\n      <td>12:33:00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'face_descriptor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mface_descriptor\u001B[49m\u001B[38;5;241m.\u001B[39mFaceDescriptorModel(download_weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, version\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mefficientnet_b1\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'face_descriptor' is not defined"
     ]
    }
   ],
   "source": [
    "model = face_descriptor.FaceDescriptorModel(download_weights=True, version=\"efficientnet_b1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "EfficientNet(\n  (features): Sequential(\n    (0): ConvNormActivation(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): SiLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (2): ConvNormActivation(\n            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n      )\n    )\n    (2): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n      )\n    )\n    (3): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n      )\n    )\n    (4): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n      )\n    )\n    (5): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n      )\n    )\n    (6): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n      )\n      (3): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n      )\n    )\n    (7): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n      )\n    )\n    (8): ConvNormActivation(\n      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): SiLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): Dropout(p=0.2, inplace=True)\n    (1): Linear(in_features=1280, out_features=1000, bias=True)\n  )\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((240, 240)),transforms.RandomHorizontalFlip(),transforms.RandomRotation(10),transforms.RandomAutocontrast(),data_load.Normalize()])\n",
    "test_transform=transforms.Compose([transforms.ToTensor(), transforms.Resize((240, 240)),data_load.Normalize()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "EfficientNet(\n  (features): Sequential(\n    (0): ConvNormActivation(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): SiLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (2): ConvNormActivation(\n            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (2): ConvNormActivation(\n            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n      )\n    )\n    (2): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n      )\n    )\n    (3): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n      )\n    )\n    (4): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n      )\n      (3): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n      )\n    )\n    (5): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n      )\n      (3): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n      )\n    )\n    (6): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n      )\n      (3): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n      )\n      (4): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n      )\n    )\n    (7): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n            (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n      )\n    )\n    (8): ConvNormActivation(\n      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): SiLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): Dropout(p=0.2, inplace=True)\n    (1): Linear(in_features=1280, out_features=1000, bias=True)\n  )\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100.0 % of the folders processed img dict loaded in 0.26 m\n",
      " 100.0 % of the folders processed img dict loaded in 0.05 m\n"
     ]
    }
   ],
   "source": [
    "train_dataset = data_load.FacesDataset(f\"{dataset_path}/train\", 5000, train_transform,True)\n",
    "test_dataset=data_load.FacesDataset(f\"{dataset_path}/test\", 1000, test_transform,True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_dataset.no_of_rows=10000\n",
    "test_dataset.no_of_rows=2500"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "a_batch, p_batch, n_batch = next(iter(train_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 240, 240])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_batch.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(30):\n",
    "#     _,ax=plt.subplots(1,3)\n",
    "#     ax[0].set_title(\"Anchor\")\n",
    "#     ax[0].imshow(a[i])\n",
    "#\n",
    "#     ax[1].set_title(\"Positive\")\n",
    "#     ax[1].imshow(p[i])\n",
    "#\n",
    "#     ax[2].set_title(\"Negative\")\n",
    "#     ax[2].imshow(n[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1],\n        [2]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "x=torch.tensor([1,2])\n",
    "x.unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ts=time.time()\n",
    "a_batch, p_batch, n_batch = next(iter(train_loader))\n",
    "print(time.time()-ts)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(batch_size):\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3)\n",
    "\n",
    "        # anchor_vector=model(a_batch[i].unsqueeze(0)).detach()[0]\n",
    "        # positive_vector=model(p_batch[i].unsqueeze(0)).detach()[0]\n",
    "        # negative_vector=model(n_batch[i].unsqueeze(0)).detach()[0]\n",
    "\n",
    "        ax[0].set_title(\"anchor\")\n",
    "        ax[0].imshow(a_batch[i].numpy().transpose([1,2,0]))\n",
    "\n",
    "\n",
    "        ax[1].set_title(\"positive\")\n",
    "        ax[1].imshow(p_batch[i].numpy().transpose([1,2,0]))\n",
    "\n",
    "\n",
    "\n",
    "        ax[2].set_title(\"negative\")\n",
    "        ax[2].imshow(n_batch[i].numpy().transpose([1,2,0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"anchor vs Positive = {utils.euclidean_distance(anchor_vector.numpy(),positive_vector.numpy())}\")\n",
    "        # print(f\"anchor vs negative = {utils.euclidean_distance(anchor_vector.numpy(),negative_vector.numpy())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "for i in range (len(model.features)):\n",
    "      for parm in model.features[i].parameters():\n",
    "            parm.requires_grad=True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "for i in range(stop_n_layers):\n",
    "  for parm in model.features[i].parameters():\n",
    "    parm.requires_grad=True\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "epochs=15\n",
    "learning_rate=0.001\n",
    "train_notes=\"Classifier -->(1280,256,128)+ full train , randomHorizontal flip,random Rotation , randomAutoContrast,effnetb1 , learnrate =0.0001\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "model.load_local_weights(\"../model_weights/training/05_20 18_23 Train_(0.164040) Test_(0.167041).pt\",True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing before training ...\n",
      " epoch 1 [==========] time remaining = 0.0 Avg Train_Loss=0.16056560822\n",
      " Testing  [==========] time remaining = 0.0 Avg Test_Loss=0.16681166669\n",
      " epoch 1 train_loss =0.16056545398235322 test_loss=0.16681104350090026\n",
      "new minimum test loss 0.166811  achieved, model weights saved \n",
      "!!!Warning Overfitting!!!\n",
      " epoch 2 [==========] time remaining = 0.0 Avg Train_Loss=0.16182660915\n",
      " Testing  [==========] time remaining = 0.0 Avg Test_Loss=0.18263180268\n",
      " epoch 2 train_loss =0.16182647721767426 test_loss=0.18263150191307068\n",
      "!!!Warning Overfitting!!!\n",
      " epoch 3 [==........] time remaining = 5.139126 Avg Train_Loss=0.160807"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_losses\u001B[38;5;241m=\u001B[39m\u001B[43mmodels_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtriplet_loss_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mlearn_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0001\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43mweight_saving_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_weights_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43mepoch_data_saving_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_data_save_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43mnotes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_notes\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\src\\efficentnet_train\\models_train.py:72\u001B[0m, in \u001B[0;36mtriplet_loss_train\u001B[1;34m(model, epochs, learn_rate, train_loader, test_loader, cuda, weight_saving_path, epoch_data_saving_path, notes)\u001B[0m\n\u001B[0;32m     69\u001B[0m loss_sum \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m     71\u001B[0m cnt \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m---> 72\u001B[0m time_sum \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m anchor_img, positive_img, negative_img \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m     74\u001B[0m     batch_start_t \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    528\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    529\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[1;32m--> 530\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    531\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    532\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    533\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    534\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    569\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 570\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    572\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:32\u001B[0m, in \u001B[0;36m_IterableDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index:\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 32\u001B[0m         data\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset_iter\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[0;32m     34\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mended \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\src\\efficentnet_train\\data_load.py:187\u001B[0m, in \u001B[0;36mFacesDataset.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    184\u001B[0m n_img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload_img(n_img_name)\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 187\u001B[0m     a_img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma_img\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    188\u001B[0m     p_img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(p_img)\n\u001B[0;32m    189\u001B[0m     n_img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(n_img)\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:1349\u001B[0m, in \u001B[0;36mRandomRotation.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m   1346\u001B[0m         fill \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mfloat\u001B[39m(f) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m fill]\n\u001B[0;32m   1347\u001B[0m angle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_params(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdegrees)\n\u001B[1;32m-> 1349\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrotate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mangle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpand\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcenter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:1078\u001B[0m, in \u001B[0;36mrotate\u001B[1;34m(img, angle, interpolation, expand, center, fill, resample)\u001B[0m\n\u001B[0;32m   1075\u001B[0m \u001B[38;5;66;03m# due to current incoherence of rotation angle direction between affine and rotate implementations\u001B[39;00m\n\u001B[0;32m   1076\u001B[0m \u001B[38;5;66;03m# we need to set -angle.\u001B[39;00m\n\u001B[0;32m   1077\u001B[0m matrix \u001B[38;5;241m=\u001B[39m _get_inverse_affine_matrix(center_f, \u001B[38;5;241m-\u001B[39mangle, [\u001B[38;5;241m0.0\u001B[39m, \u001B[38;5;241m0.0\u001B[39m], \u001B[38;5;241m1.0\u001B[39m, [\u001B[38;5;241m0.0\u001B[39m, \u001B[38;5;241m0.0\u001B[39m])\n\u001B[1;32m-> 1078\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_t\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrotate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmatrix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmatrix\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterpolation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexpand\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexpand\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfill\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\venv\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:686\u001B[0m, in \u001B[0;36mrotate\u001B[1;34m(img, matrix, interpolation, expand, fill)\u001B[0m\n\u001B[0;32m    683\u001B[0m \u001B[38;5;66;03m# grid will be generated on the same device as theta and img\u001B[39;00m\n\u001B[0;32m    684\u001B[0m grid \u001B[38;5;241m=\u001B[39m _gen_affine_grid(theta, w\u001B[38;5;241m=\u001B[39mw, h\u001B[38;5;241m=\u001B[39mh, ow\u001B[38;5;241m=\u001B[39mow, oh\u001B[38;5;241m=\u001B[39moh)\n\u001B[1;32m--> 686\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_apply_grid_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfill\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfill\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\venv\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:586\u001B[0m, in \u001B[0;36m_apply_grid_transform\u001B[1;34m(img, grid, mode, fill)\u001B[0m\n\u001B[0;32m    583\u001B[0m     dummy \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mones((img\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m1\u001B[39m, img\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m], img\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m3\u001B[39m]), dtype\u001B[38;5;241m=\u001B[39mimg\u001B[38;5;241m.\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39mimg\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m    584\u001B[0m     img \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((img, dummy), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 586\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43mgrid_sample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mzeros\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malign_corners\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    588\u001B[0m \u001B[38;5;66;03m# Fill with required color\u001B[39;00m\n\u001B[0;32m    589\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fill \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\venv\\lib\\site-packages\\torch\\nn\\functional.py:4201\u001B[0m, in \u001B[0;36mgrid_sample\u001B[1;34m(input, grid, mode, padding_mode, align_corners)\u001B[0m\n\u001B[0;32m   4193\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   4194\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDefault grid_sample and affine_grid behavior has changed \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   4195\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto align_corners=False since 1.3.0. Please specify \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   4196\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124malign_corners=True if the old behavior is desired. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   4197\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSee the documentation of grid_sample for details.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   4198\u001B[0m     )\n\u001B[0;32m   4199\u001B[0m     align_corners \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m-> 4201\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrid_sampler\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode_enum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_mode_enum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malign_corners\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_losses=models_train.triplet_loss_train(model,epochs=1000,learn_rate=0.0001,train_loader=train_loader,test_loader=test_loader,cuda=True,weight_saving_path=model_weights_path,epoch_data_saving_path=train_data_save_path,notes=train_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_notes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_losses\u001B[38;5;241m=\u001B[39mmodels_train\u001B[38;5;241m.\u001B[39mtriplet_loss_train(model,epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m,learn_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0001\u001B[39m,train_loader\u001B[38;5;241m=\u001B[39mtrain_loader,test_loader\u001B[38;5;241m=\u001B[39mtest_loader,cuda\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,weight_saving_path\u001B[38;5;241m=\u001B[39mmodel_weights_path,epoch_data_saving_path\u001B[38;5;241m=\u001B[39mtrain_data_save_path,notes\u001B[38;5;241m=\u001B[39m\u001B[43mtrain_notes\u001B[49m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_notes' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "saving_path=model_weights_path,epoch_data_saving_path=train_data_save_path,notes=train_notes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "model.load_local_weights(model_weights_path+\"/05_19 05_11 Train_(0.203699) Test_(0.367653).pt\",cuda_weights=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data processed [==========] time remaing=0.006"
     ]
    }
   ],
   "source": [
    "features_dict=data_load.get_pic_features_dict(f\"{dataset_path}/test\",model,test_transform,cuda=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "type(np.array([[1,2,3]]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "test_data_pd=pd.DataFrame(generate_dataset.generate_testing_data_set_frame(dataset_path+\"/test\",True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing  [=====.....] time remaining = 0.037384 Accuracy =87.013"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [26]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mevaluation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures_dict\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtest_data_pd\u001B[49m\u001B[43m,\u001B[49m\u001B[43mresults_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mthreshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3.9\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\src\\efficentnet_train\\evaluation.py:32\u001B[0m, in \u001B[0;36mmodel_test\u001B[1;34m(features_vectors_dict, dataset_frame, threshold, results_path)\u001B[0m\n\u001B[0;32m     29\u001B[0m emp1 \u001B[38;5;241m=\u001B[39m features_vectors_dict[img1Path]\n\u001B[0;32m     30\u001B[0m emp2 \u001B[38;5;241m=\u001B[39m features_vectors_dict[img2Path]\n\u001B[1;32m---> 32\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43meuclidean_distance\u001B[49m\u001B[43m(\u001B[49m\u001B[43memp1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43memp2\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m pred_dist \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     34\u001B[0m actual_dist \u001B[38;5;241m=\u001B[39m row[\u001B[38;5;241m2\u001B[39m]\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\src\\efficentnet_train\\utils.py:21\u001B[0m, in \u001B[0;36meuclidean_distance\u001B[1;34m(vector_a, vector_b)\u001B[0m\n\u001B[0;32m     19\u001B[0m diff \u001B[38;5;241m=\u001B[39m vector_a \u001B[38;5;241m-\u001B[39m vector_b\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# sum squared\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m sum_squared \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdiff\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdiff\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39msqrt(sum_squared)\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mdot\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "evaluation.model_test(features_dict,test_data_pd,results_path=\"\",threshold=3.9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "7.0710678118654755"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.euclidean_distance(np.array([5,2,3]),[1,5,8])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}