{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from src.efficentnet_train import data_load,models_train,visualization,utils,generate_dataset,evaluation\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from src.efficentfacenet import face_descriptor\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = \"../dataset/preprocessed\"\n",
    "model_weights_path=\"../model_weights/training\"\n",
    "train_data_save_path=\"../training log\"\n",
    "batch_size=8\n",
    "stop_n_layers=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_history=pd.read_csv(train_data_save_path+\"/train_data.csv\",index_col=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "      Train Loss  no train rows  Test Loss  No test rows  Time taken (M)  \\\n0       0.280173         100000   0.342320         25000            65.2   \n1       0.989897         100000   1.000320         25000            60.1   \n2       1.000000         100000   1.000320         25000            60.2   \n3       0.173325          50000   0.284327         10000            63.0   \n4       0.243919          50000   0.281481         10000            60.0   \n5       0.210516          50000   0.238477         10000            59.9   \n6       0.122000          50000   0.259760         10000            62.9   \n7       0.197626          50000   0.260942         10000            60.1   \n8       0.168668          50000   0.216822         10000            60.2   \n9       0.097149          25000   0.268254          5000            31.3   \n10      0.348466          50000   0.329321         10000            77.7   \n11      0.195241          50000   0.275927         10000            83.4   \n12      0.996439          50000   1.000000         10000            79.3   \n13      1.000000          50000   1.000000         10000            79.3   \n14      1.000000          50000   1.000000         10000            79.4   \n15      1.000000          50000   1.000000         10000            79.6   \n16      0.652350           5000   0.515885         10000            12.7   \n17  31245.067151           5000   1.000000         10000            12.3   \n18      1.000000           5000   1.000000         10000            12.5   \n19      1.000000           5000   1.000000         10000            12.5   \n20      1.000000           5000   1.000000         10000            12.4   \n21      0.688501           5000   0.508890         10000            11.9   \n22    557.757865           5000   1.000000         10000            11.3   \n23      1.000000           5000   1.000000         10000            11.0   \n24      1.000000           5000   1.000000         10000            10.9   \n25      1.000000           5000   1.000000         10000            10.9   \n26      1.000000           5000   1.000000         10000            11.0   \n27      1.000000           5000   1.000000         10000            11.1   \n28      1.000000           5000   1.000000         10000            11.0   \n29      1.000000           5000   1.000000         10000            11.0   \n30      1.000000           5000   1.000000         10000            11.0   \n31      0.468498          25000   0.351068          5000            36.2   \n32      1.876949          25000   1.000000          5000            33.3   \n33      0.797767           5000   0.654682          5000             9.4   \n34      0.990279           5000   1.000000          5000             8.5   \n35      1.000000           5000   1.000000          5000             8.5   \n36      1.000000           5000   1.000000          5000             8.5   \n37      1.000000           5000   1.000000          5000             8.5   \n38      1.000000           5000   1.000000          5000             8.5   \n39      1.000000           5000   1.000000          5000             8.5   \n40      1.000000           5000   1.000000          5000             8.5   \n41      1.000000           5000   1.000000          5000             8.5   \n42      1.000000           5000   1.000000          5000             8.5   \n43      0.584966          25000   0.429947          5000            67.8   \n\n                                                Notes        Date      Time  \n0   remove sigmoid from classifier + input normali...  15/05/2022  23:26:00  \n1   remove sigmoid from classifier + input normali...  16/05/2022  00:27:00  \n2   remove sigmoid from classifier + input normali...  16/05/2022  01:27:00  \n3   remove sigmoid from classifier + input normali...  16/05/2022  19:28:00  \n4   remove sigmoid from classifier + input normali...  16/05/2022  20:28:00  \n5   remove sigmoid from classifier + input normali...  16/05/2022  21:28:00  \n6   remove sigmoid from classifier + input normali...  16/05/2022  22:38:00  \n7   remove sigmoid from classifier + input normali...  16/05/2022  23:38:00  \n8   remove sigmoid from classifier + input normali...  17/05/2022  00:38:00  \n9     Add sigmoid to classifier + input normalization  17/05/2022  14:29:00  \n10  Classifier -->(1280,128,sigmoid) + input stand...  17/05/2022  18:32:00  \n11  Classifier -->(1280,128,sigmoid) + input stand...  17/05/2022  20:02:00  \n12  Classifier -->(1280,128,sigmoid) + input stand...  17/05/2022  21:22:00  \n13  Classifier -->(1280,128,sigmoid) + input stand...  17/05/2022  22:41:00  \n14  Classifier -->(1280,128,sigmoid) + input stand...  18/05/2022  00:00:00  \n15  Classifier -->(1280,128,sigmoid) + input stand...  18/05/2022  01:20:00  \n16  Classifier -->(1280,128) + input standardizati...  18/05/2022  02:33:00  \n17  Classifier -->(1280,128) + input standardizati...  18/05/2022  02:45:00  \n18  Classifier -->(1280,128) + input standardizati...  18/05/2022  02:58:00  \n19  Classifier -->(1280,128) + input standardizati...  18/05/2022  03:10:00  \n20  Classifier -->(1280,128) + input standardizati...  18/05/2022  03:23:00  \n21  Classifier -->(1280,128) + input standardizati...  18/05/2022  03:39:00  \n22  Classifier -->(1280,128) + input standardizati...  18/05/2022  03:50:00  \n23  Classifier -->(1280,128) + input standardizati...  18/05/2022  04:01:00  \n24  Classifier -->(1280,128) + input standardizati...  18/05/2022  04:12:00  \n25  Classifier -->(1280,128) + input standardizati...  18/05/2022  04:23:00  \n26  Classifier -->(1280,128) + input standardizati...  18/05/2022  04:34:00  \n27  Classifier -->(1280,128) + input standardizati...  18/05/2022  04:45:00  \n28  Classifier -->(1280,128) + input standardizati...  18/05/2022  04:56:00  \n29  Classifier -->(1280,128) + input standardizati...  18/05/2022  05:07:00  \n30  Classifier -->(1280,128) + input standardizati...  18/05/2022  05:18:00  \n31  Classifier -->(1280,128) + input standardizati...  18/05/2022  07:17:00  \n32  Classifier -->(1280,128) + input standardizati...  18/05/2022  07:51:00  \n33  Classifier -->(1280,128) + input standardizati...  18/05/2022  08:17:00  \n34  Classifier -->(1280,128) + input standardizati...  18/05/2022  08:26:00  \n35  Classifier -->(1280,128) + input standardizati...  18/05/2022  08:34:00  \n36  Classifier -->(1280,128) + input standardizati...  18/05/2022  08:43:00  \n37  Classifier -->(1280,128) + input standardizati...  18/05/2022  08:51:00  \n38  Classifier -->(1280,128) + input standardizati...  18/05/2022  09:00:00  \n39  Classifier -->(1280,128) + input standardizati...  18/05/2022  09:08:00  \n40  Classifier -->(1280,128) + input standardizati...  18/05/2022  09:17:00  \n41  Classifier -->(1280,128) + input standardizati...  18/05/2022  09:25:00  \n42  Classifier -->(1280,128) + input standardizati...  18/05/2022  09:34:00  \n43  Classifier -->(1280,512,128)+ sigmoid + input ...  18/05/2022  12:33:00  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Train Loss</th>\n      <th>no train rows</th>\n      <th>Test Loss</th>\n      <th>No test rows</th>\n      <th>Time taken (M)</th>\n      <th>Notes</th>\n      <th>Date</th>\n      <th>Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.280173</td>\n      <td>100000</td>\n      <td>0.342320</td>\n      <td>25000</td>\n      <td>65.2</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>15/05/2022</td>\n      <td>23:26:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.989897</td>\n      <td>100000</td>\n      <td>1.000320</td>\n      <td>25000</td>\n      <td>60.1</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>16/05/2022</td>\n      <td>00:27:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.000000</td>\n      <td>100000</td>\n      <td>1.000320</td>\n      <td>25000</td>\n      <td>60.2</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>16/05/2022</td>\n      <td>01:27:00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.173325</td>\n      <td>50000</td>\n      <td>0.284327</td>\n      <td>10000</td>\n      <td>63.0</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>16/05/2022</td>\n      <td>19:28:00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.243919</td>\n      <td>50000</td>\n      <td>0.281481</td>\n      <td>10000</td>\n      <td>60.0</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>16/05/2022</td>\n      <td>20:28:00</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.210516</td>\n      <td>50000</td>\n      <td>0.238477</td>\n      <td>10000</td>\n      <td>59.9</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>16/05/2022</td>\n      <td>21:28:00</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.122000</td>\n      <td>50000</td>\n      <td>0.259760</td>\n      <td>10000</td>\n      <td>62.9</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>16/05/2022</td>\n      <td>22:38:00</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.197626</td>\n      <td>50000</td>\n      <td>0.260942</td>\n      <td>10000</td>\n      <td>60.1</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>16/05/2022</td>\n      <td>23:38:00</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.168668</td>\n      <td>50000</td>\n      <td>0.216822</td>\n      <td>10000</td>\n      <td>60.2</td>\n      <td>remove sigmoid from classifier + input normali...</td>\n      <td>17/05/2022</td>\n      <td>00:38:00</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.097149</td>\n      <td>25000</td>\n      <td>0.268254</td>\n      <td>5000</td>\n      <td>31.3</td>\n      <td>Add sigmoid to classifier + input normalization</td>\n      <td>17/05/2022</td>\n      <td>14:29:00</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.348466</td>\n      <td>50000</td>\n      <td>0.329321</td>\n      <td>10000</td>\n      <td>77.7</td>\n      <td>Classifier --&gt;(1280,128,sigmoid) + input stand...</td>\n      <td>17/05/2022</td>\n      <td>18:32:00</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.195241</td>\n      <td>50000</td>\n      <td>0.275927</td>\n      <td>10000</td>\n      <td>83.4</td>\n      <td>Classifier --&gt;(1280,128,sigmoid) + input stand...</td>\n      <td>17/05/2022</td>\n      <td>20:02:00</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.996439</td>\n      <td>50000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>79.3</td>\n      <td>Classifier --&gt;(1280,128,sigmoid) + input stand...</td>\n      <td>17/05/2022</td>\n      <td>21:22:00</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1.000000</td>\n      <td>50000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>79.3</td>\n      <td>Classifier --&gt;(1280,128,sigmoid) + input stand...</td>\n      <td>17/05/2022</td>\n      <td>22:41:00</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>1.000000</td>\n      <td>50000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>79.4</td>\n      <td>Classifier --&gt;(1280,128,sigmoid) + input stand...</td>\n      <td>18/05/2022</td>\n      <td>00:00:00</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>1.000000</td>\n      <td>50000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>79.6</td>\n      <td>Classifier --&gt;(1280,128,sigmoid) + input stand...</td>\n      <td>18/05/2022</td>\n      <td>01:20:00</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.652350</td>\n      <td>5000</td>\n      <td>0.515885</td>\n      <td>10000</td>\n      <td>12.7</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>02:33:00</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>31245.067151</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>12.3</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>02:45:00</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>12.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>02:58:00</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>12.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>03:10:00</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>12.4</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>03:23:00</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.688501</td>\n      <td>5000</td>\n      <td>0.508890</td>\n      <td>10000</td>\n      <td>11.9</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>03:39:00</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>557.757865</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>11.3</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>03:50:00</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>11.0</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>04:01:00</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>10.9</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>04:12:00</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>10.9</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>04:23:00</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>11.0</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>04:34:00</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>11.1</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>04:45:00</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>11.0</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>04:56:00</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>11.0</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>05:07:00</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>10000</td>\n      <td>11.0</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>05:18:00</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.468498</td>\n      <td>25000</td>\n      <td>0.351068</td>\n      <td>5000</td>\n      <td>36.2</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>07:17:00</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>1.876949</td>\n      <td>25000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>33.3</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>07:51:00</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.797767</td>\n      <td>5000</td>\n      <td>0.654682</td>\n      <td>5000</td>\n      <td>9.4</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>08:17:00</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.990279</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>08:26:00</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>08:34:00</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>08:43:00</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>08:51:00</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>09:00:00</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>09:08:00</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>09:17:00</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>09:25:00</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>1.000000</td>\n      <td>5000</td>\n      <td>8.5</td>\n      <td>Classifier --&gt;(1280,128) + input standardizati...</td>\n      <td>18/05/2022</td>\n      <td>09:34:00</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>0.584966</td>\n      <td>25000</td>\n      <td>0.429947</td>\n      <td>5000</td>\n      <td>67.8</td>\n      <td>Classifier --&gt;(1280,512,128)+ sigmoid + input ...</td>\n      <td>18/05/2022</td>\n      <td>12:33:00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = face_descriptor.FaceDescriptorModel(download_weights=True, version=\"efficientnet_b1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "EfficientNet(\n  (features): Sequential(\n    (0): ConvNormActivation(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): SiLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (2): ConvNormActivation(\n            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n      )\n    )\n    (2): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n      )\n    )\n    (3): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n      )\n    )\n    (4): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n      )\n    )\n    (5): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n      )\n    )\n    (6): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n      )\n      (3): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n      )\n    )\n    (7): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n      )\n    )\n    (8): ConvNormActivation(\n      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): SiLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): Dropout(p=0.2, inplace=True)\n    (1): Linear(in_features=1280, out_features=1000, bias=True)\n  )\n)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((240, 240)),transforms.RandomHorizontalFlip(),transforms.RandomRotation(10),transforms.RandomAutocontrast(),data_load.Normalize()])\n",
    "test_transform=transforms.Compose([transforms.ToTensor(), transforms.Resize((240, 240)),data_load.Normalize()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "EfficientNet(\n  (features): Sequential(\n    (0): ConvNormActivation(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): SiLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (2): ConvNormActivation(\n            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (2): ConvNormActivation(\n            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n      )\n    )\n    (2): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n      )\n    )\n    (3): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n      )\n    )\n    (4): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n      )\n      (3): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n      )\n    )\n    (5): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n      )\n      (3): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n      )\n    )\n    (6): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n      )\n      (3): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n      )\n      (4): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n      )\n    )\n    (7): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): ConvNormActivation(\n            (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): ConvNormActivation(\n            (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n            (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): ConvNormActivation(\n            (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n      )\n    )\n    (8): ConvNormActivation(\n      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): SiLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): Dropout(p=0.2, inplace=True)\n    (1): Linear(in_features=1280, out_features=1000, bias=True)\n  )\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100.0 % of the folders processed img dict loaded in 0.34 m\n",
      " 100.0 % of the folders processed img dict loaded in 0.07 m\n"
     ]
    }
   ],
   "source": [
    "train_dataset = data_load.FacesDataset(f\"{dataset_path}/train\", 5000, train_transform,True)\n",
    "test_dataset=data_load.FacesDataset(f\"{dataset_path}/test\", 1000, test_transform,True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "train_dataset.no_of_rows=10000\n",
    "test_dataset.no_of_rows=2500"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "a_batch, p_batch, n_batch = next(iter(train_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 240, 240])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_batch.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# for i in range(30):\n",
    "#     _,ax=plt.subplots(1,3)\n",
    "#     ax[0].set_title(\"Anchor\")\n",
    "#     ax[0].imshow(a[i])\n",
    "#\n",
    "#     ax[1].set_title(\"Positive\")\n",
    "#     ax[1].imshow(p[i])\n",
    "#\n",
    "#     ax[2].set_title(\"Negative\")\n",
    "#     ax[2].imshow(n[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1],\n        [2]])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "x=torch.tensor([1,2])\n",
    "x.unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ts=time.time()\n",
    "a_batch, p_batch, n_batch = next(iter(train_loader))\n",
    "print(time.time()-ts)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(batch_size):\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3)\n",
    "\n",
    "        # anchor_vector=model(a_batch[i].unsqueeze(0)).detach()[0]\n",
    "        # positive_vector=model(p_batch[i].unsqueeze(0)).detach()[0]\n",
    "        # negative_vector=model(n_batch[i].unsqueeze(0)).detach()[0]\n",
    "\n",
    "        ax[0].set_title(\"anchor\")\n",
    "        ax[0].imshow(a_batch[i].numpy().transpose([1,2,0]))\n",
    "\n",
    "\n",
    "        ax[1].set_title(\"positive\")\n",
    "        ax[1].imshow(p_batch[i].numpy().transpose([1,2,0]))\n",
    "\n",
    "\n",
    "\n",
    "        ax[2].set_title(\"negative\")\n",
    "        ax[2].imshow(n_batch[i].numpy().transpose([1,2,0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # print(f\"anchor vs Positive = {utils.euclidean_distance(anchor_vector.numpy(),positive_vector.numpy())}\")\n",
    "        # print(f\"anchor vs negative = {utils.euclidean_distance(anchor_vector.numpy(),negative_vector.numpy())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "for i in range (len(model.features)):\n",
    "      for parm in model.features[i].parameters():\n",
    "            parm.requires_grad=True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "for i in range(stop_n_layers):\n",
    "  for parm in model.features[i].parameters():\n",
    "    parm.requires_grad=True\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "epochs=15\n",
    "learning_rate=0.001\n",
    "train_notes=\"Classifier -->(1280,256,128)+ full train , randomHorizontal flip,random Rotation , randomAutoContrast,effnetb1 , learnrate =0.0001\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model.load_local_weights(\"../model_weights/training/05_20 19_05 Train_(0.160565) Test_(0.166811).pt\",True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing before training ...\n",
      " Testing  [==========] time remaining = 0.0 Avg Test_Loss=0.150402Test Loss before Training=0.15064265232086183\n",
      " epoch 1 [==========] time remaining = 0.0 Avg Train_Loss=0.09595096027\n",
      " Testing  [==========] time remaining = 0.0 Avg Test_Loss=0.15975860270\n",
      " epoch 1 train_loss =0.09595097725391388 test_loss=0.16001451511383058\n",
      "!!!Warning Overfitting!!!\n",
      " epoch 2 [==========] time remaining = 0.0 Avg Train_Loss=0.09915699235\n",
      " Testing  [==========] time remaining = 0.0 Avg Test_Loss=0.17296072805\n",
      " epoch 2 train_loss =0.09915616257190704 test_loss=0.17323749313354492\n",
      "!!!Warning Overfitting!!!\n",
      " epoch 3 [==========] time remaining = 0.0 Avg Train_Loss=0.09847398552\n",
      " Testing  [==========] time remaining = 0.0 Avg Test_Loss=0.15139051876\n",
      " epoch 3 train_loss =0.09847376960515976 test_loss=0.15163300457000733\n",
      "!!!Warning Overfitting!!!\n",
      " epoch 4 [==========] time remaining = 0.0 Avg Train_Loss=0.09322093294\n",
      " Testing  [==========] time remaining = 0.0 Avg Test_Loss=0.16142661943\n",
      " epoch 4 train_loss =0.09322000434398652 test_loss=0.1616843719482422\n",
      "!!!Warning Overfitting!!!\n",
      " epoch 5 [==========] time remaining = 0.0 Avg Train_Loss=0.09914199220\n",
      " Testing  [==========] time remaining = 0.0 Avg Test_Loss=0.12118821576\n",
      " epoch 5 train_loss =0.09914102206230163 test_loss=0.12138191385269165\n",
      "new minimum test loss 0.121381  achieved, model weights saved \n",
      "!!!Warning Overfitting!!!\n",
      " epoch 6 [==========] time remaining = 0.0 Avg Train_Loss=0.08411584182\n",
      " Testing  [==========] time remaining = 0.0 Avg Test_Loss=0.14802248496\n",
      " epoch 6 train_loss =0.0841156488418579 test_loss=0.1482592435836792\n",
      "!!!Warning Overfitting!!!\n",
      " epoch 7 [======....] time remaining = 5.271472 Avg Train_Loss=0.095145"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [27]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_losses\u001B[38;5;241m=\u001B[39m\u001B[43mmodels_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtriplet_loss_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mlearn_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0001\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43mweight_saving_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_weights_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43mepoch_data_saving_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_data_save_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43mnotes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_notes\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\src\\efficentnet_train\\models_train.py:88\u001B[0m, in \u001B[0;36mtriplet_loss_train\u001B[1;34m(model, epochs, learn_rate, train_loader, test_loader, cuda, weight_saving_path, epoch_data_saving_path, notes)\u001B[0m\n\u001B[0;32m     85\u001B[0m negative_vector \u001B[38;5;241m=\u001B[39m model(negative_img)\n\u001B[0;32m     87\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_function(anchor_vector, positive_vector, negative_vector)\n\u001B[1;32m---> 88\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     90\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     91\u001B[0m loss_sum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m*\u001B[39m batch_size\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\venv\\lib\\site-packages\\torch\\_tensor.py:363\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    355\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    356\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    357\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    361\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[0;32m    362\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[1;32m--> 363\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\graduation_project\\efficient-facenet\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_losses=models_train.triplet_loss_train(model,epochs=1000,learn_rate=0.0001,train_loader=train_loader,test_loader=test_loader,cuda=True,weight_saving_path=model_weights_path,epoch_data_saving_path=train_data_save_path,notes=train_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_notes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_losses\u001B[38;5;241m=\u001B[39mmodels_train\u001B[38;5;241m.\u001B[39mtriplet_loss_train(model,epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m,learn_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0001\u001B[39m,train_loader\u001B[38;5;241m=\u001B[39mtrain_loader,test_loader\u001B[38;5;241m=\u001B[39mtest_loader,cuda\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,weight_saving_path\u001B[38;5;241m=\u001B[39mmodel_weights_path,epoch_data_saving_path\u001B[38;5;241m=\u001B[39mtrain_data_save_path,notes\u001B[38;5;241m=\u001B[39m\u001B[43mtrain_notes\u001B[49m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_notes' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "saving_path=model_weights_path,epoch_data_saving_path=train_data_save_path,notes=train_notes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "model.load_local_weights(model_weights_path+\"/05_19 05_11 Train_(0.203699) Test_(0.367653).pt\",cuda_weights=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " data processed [==========] time remaing=0.006"
     ]
    }
   ],
   "source": [
    "features_dict=data_load.get_pic_features_dict(f\"{dataset_path}/test\",model,test_transform,cuda=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "type(np.array([[1,2,3]]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "test_data_pd=pd.DataFrame(generate_dataset.generate_testing_data_set_frame(dataset_path+\"/test\",True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing  [==========] time remaining = 0.0 Accuracy =86.064Accuracy now equal --> 86.0645%\n"
     ]
    },
    {
     "data": {
      "text/plain": "(                              Mertic         Value\n 0                     processed rows  88192.000000\n 1  Model accuracy on Proceed Faces %     86.064000\n 2                     False Positive  10922.000000\n 3                     False Negative   1368.000000\n 4                          precision      0.796406\n 5                             recall      0.968955\n 6                        fbeta-score      0.444916\n 7           avg same person distance      3.747763\n 8           avg diff person distance      8.288865\n 9                    Model tolerance      6.300000,\n                  Actual True  Actual False\n Predicted True         42728         10922\n Predicted False         1368         33174)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation.model_test(features_dict,test_data_pd,results_path=\"\",threshold=6.3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m evaluation\u001B[38;5;241m.\u001B[39meuclidean_distance(\u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m5\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m3\u001B[39m]),[\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m5\u001B[39m,\u001B[38;5;241m8\u001B[39m])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "evaluation.euclidean_distance(np.array([5,2,3]),[1,5,8])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}